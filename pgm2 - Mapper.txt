pgm2 - Mapper

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class M1 extends Mapper<LongWritable, Text, Text, IntWritable> {
    private static final int MISSING = 9999;

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] fields = line.split(",");
       if (fields.length > 3 && !fields[2].equals("Year")) {
            try {
                int temperature = Integer.parseInt(fields[3]);
                context.write(new Text(fields[2]), new IntWritable(temperature));
            } catch (NumberFormatException e) {
            }
        }
    }
}

pgm2 - Reducer

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class R1 extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int maxTemperature = Integer.MIN_VALUE;
        for (IntWritable value : values) {
            maxTemperature = Math.max(maxTemperature, value.get());
        }
        context.write(key, new IntWritable(maxTemperature));
    }
}

pgm2 - Driver

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class Driver {
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: MaxTemperature <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Max Temperature");

        job.setJarByClass(Driver.class);
        job.setMapperClass(M1.class);
        job.setReducerClass(R1.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}


cmds---
mkdir folder
javac -classpath $(hadoop classpath) -d folder M1.java R1.java Driver.java
jar -cvf recordcount.jar -C folder/ .
hdfs dfs -mkdir input

hdfs dfs -put dataset124.csv /input/data.csv
yarn jar recordcount.jar (<packagename>).Driver /input /output2
hdfs dfs -ls /output2
hdfs dfs -cat /output2/part-r-00000
hadoop application -status <applicationId>
hadoop job -list
hadoop job -kill $jobId

mkdir folder
javac -classpath $(hadoop classpath) -d folder RecordCountMapper.java RecordCountReducer.java RecordCount.java
jar -cvf recordcount.jar -C folder/ .
hdfs dfs -mkdir input
hdfs dfs -put dataset124.csv /input/data.csv
yarn jar recordcount.jar (<packagename>).RecordCount /input /output
hdfs dfs -ls /output
hdfs dfs -cat /output/part-r-00000


mkdir folder

javac -classpath $(hadoop classpath) -d folder RecordCountMapper.java RecordCountReducer.java RecordCount.java

jar -cvf recordcount.jar -C folder/ .

hdfs dfs -mkdir input

hdfs dfs -put dataset124.csv /input/data.csv

hadoop jar recordcount.jar RecordCount /input /output

hdfs dfs -ls /output

hdfs dfs -cat /output/part-r-00000

mapred job -kill <job_id>
